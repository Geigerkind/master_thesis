\section{Training mit Scikit-Learn}
Die Konstruktion eines Entscheidungsbaumes mit Scikit-Learn ist nicht deterministisch \cite{dymelThesis}.
Bei der Konstruktion mit CART können zwei Teilungsregeln gefunden werden, die eine gleich gute Unterteilung erzeugen.
In diesem Fall wählt Scikit-Learn zufällig eine Teilung aus.
Dadurch werden anschließende Teilungen beeinflusst, die in einen der Fälle womöglich bessere Teilungen hätten finden können.
Dieser Zufall ist steuerbar, indem der Startwert des Zufallgenerators auf einen vordefinierten Wert gesetzt wird.
\newline
\newline
Bei identischer Trainingsmenge und Konfiguration können folglich für verschiedene Startwerte unterschiedliche Modelle erzeugt werden.
Aus diesem Grund kann das Training mit einem Startwert als Monte Carlo Methode verstanden werden, d. h. wiederholtes Ausführen unter verschiedenen Startwerten erhöht die Wahrscheinlichkeit, dass das
beste Modell unter der angegebenen Konfiguration gefunden wird.
\newline
\newline
Dies bedarf, dass die Klassifizierungsgenauigkeit beim Training bereits ermittelt wird, damit das beste Modell ausgewählt werden kann.
Dafür wird die Trainingsmenge in zwei Mengen unterteilt. Mit einer Teilmenge werden die Entscheidungsbäume unter verschidenen Startwerten des Zufallgenerators trainiert und
mit der anderen Teilmenge werden diese untereinander verglichen. Das Ergebnis des Trainingprozesses ist das Modell, dass auf letzterer Teilmenge die höchste
Klassifizierungsgenauigkeit erzielt.