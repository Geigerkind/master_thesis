\section{Ressourcennutzung}
Bei dir Ressourcennutzung muss einerseits der benötigte Programmspeicher, der benötigte RAM, sowie die Ausführungszeit der ML-Modelle betrachtet werden.
Außerdem muss für den Energiebedarf abgewogen werden, was die Ausführungsfrequenz in bestimmten Szenarien ist.
\newline
\newline
Für die Evaluierung wird eine 32-Bit CPU mit Festkommarithmetik angenommen.

\subsection{Programmspeicher}
Der Großteil des Programmspeichers wird für das ML-Modell benötigt.
Aus diesem Grund wird der Anteil des Programmspeichers $X$ in der Evaluation vernachlässigt,
der für die restlichen Funktionen und für die Feature-Extrahierung benötigt wird.
Zudem ist der benötigte Programmspeicher dieses Anteils konstant und skaliert nicht mit der Größe, wie die ML-Modelle.
\newline
\newline
Zur Estimierung des Programmspeichers der Entscheidungsbäume wird der hybride Ansatz mit einer Toleranz von $\epsilon=0$ angenommen,
d. h. es werden für eindeutige Ergebnisse diskrete Rückgaben zurückgegeben, anstatt der Wahrscheinlichkeitsverteilung.
Als Datentyp für die Vergleiche und allen Features wird angenommen, dass ein vier Byte Datentyp verwendet wird.
Für einen Vergleich werden fünf Instruktionen benötigt \cite{dymelThesis}.
Für eine Rückgabe werden zwischen zwei und $2(N+1)$ Instruktionen und zwischen 0 und $2N$ Parameter benötigt,
wobei $N$ die Anzahl der möglichen Standorte ist.
Die Größe einer Instruktion ist 4 Byte, da eine 32-Bit CPU angenommen wird.
Die Größe des zusammenfassenden Klassifizierers $Z$ wird vernachlässigt.
\newline
\newline
Für die FFNNs wird ebenfalls ein vier Byte Datentyp für die Biase und die Gewichte angenommen.
Die Größe des Algorithmus zur Ausführung des KNN ist unbekannt und wird als konstanter Wert $Y$ angenommen,
liegt aber, den Zahlen in Gieses Arbeit nach zu urteilen, zwischen 6 und 7 KB \cite{gieseThesis}.
\newline
\newline
Tabelle \ref{tab:predictions_by_loc_size} zeigt Estimierungen des benötigten Programmspeichers der verschiedenen Konfigurationen der ML-Modelle,
wobei die konstanten Anteile vernachlässigt werden.
Potentielle Optimierungen, z. B. durch den Compiler, wurden dabei nicht betrachtet,
sowie potentielle Optimierungen des FFNN, wie Giese sie vorgeschlagen hat \cite{gieseThesis}.
Giese hat mit dem CSC-MA-Bit Format die Programmgröße um 39\% reduzieren können.
Kompilierung mit der Optimierungsstufe \textit{O2} konnte experimentell generierten C-Code
eines Entscheidungswaldes um bis zu 21,3\% reduzieren.
\newline
\newline
Der benötigte Programmspeicher beider ML-Modelle skaliert mit der Anzahl der zu klassifizierenden Standorte, der Anzahl der Schichten bzw. Bäume
und der Anzahl der Neuronen pro Schicht bzw. der maximalen Baumhöhe.
Der von den Entscheidungswäldern benötigte Programmspeicher ist für fast alle Fälle zu viel
für die Limitierungen eines kleinen eingebetteten Systems.
Die FFNNs hingegen benötigen deutlich weniger Programmspeicher und könnten innerhalb der Limitierungen der kleinen eingebetten Systeme passen.
\newline
\newline
TODO: Braucht man mehr Neuronen/Hidden Layer mit steigender Ort Anzahl? (Hier oder bei ML-Modell FFNN)
\newline
TODO: Wie viel Speicherverbrauch spart man durch die Feature-Selection zusätzlich ein?
\begin{table}[h!]
    \hspace{-0.75cm}
    \begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | }
        \hline
        \multicolumn{2}{ | l |}{Größe in KB über Standorte} & 9 & 16 & 17 & 25 & 32 & 48 & 52 & 102 \\\hline
        \multicolumn{10}{| l |}{\textbf{Entscheidungswälder}}\\\hline
        Waldgröße & Max. Baumgröße & \multicolumn{8}{ c |}{}\\\hline
        16 & 8 & 79.9 & 83.2 & 117.9 & 169.6 & 147.2 & 204.1 & 254.9 & 354.0 \\\hline
        16 & 16 & 192.2 & 199.0 & 277.4 & 512.7 & 371.1 & 750.4 & 914.4 & 1350.0 \\\hline
        16 & 32 & 185.7 & 197.6 & 287.2 & 550.2 & 394.5 & 875.6 & 1016.7 & 1582.3 \\\hline
        16 & 64 & 185.7 & 197.6 & 287.2 & 543.0 & 394.5 & 875.6 & 943.6 & 1582.3 \\\hline
        8 & 32 & 94.7 & 108.1 & 136.7 & 252.7 & 192.9 & 436.8 & 465.6 & 753.8 \\\hline
        16 & 32 & 185.7 & 197.6 & 287.2 & 550.2 & 394.5 & 875.6 & 1016.7 & 1582.3 \\\hline
        32 & 32 & 364.8 & 401.7 & 575.7 & 1055.9 & 803.9 & 1701.0 & 1962.6 & 3061.1 \\\hline
        64 & 32 & 776.1 & 842.8 & 1173.8 & 2132.7 & 1584.7 & 3455.2 & 3987.8 & 6327.0 \\\hline
        32 & 64 & 364.8 & 401.7 & 575.7 & 1055.9 & 803.9 & 1701.0 & 1962.6 & 3215.9 \\\hline
        \multicolumn{10}{| l |}{\textbf{Feed Forward neuronale Netzwerke}}\\\hline
        \#Schichten & \#Neuronen & \multicolumn{8}{ c |}{}\\\hline
        1 & 16 & 2.8 & 3.2 & 3.3 & 3.8 & 4.2 & 5.2 & 5.5 & 8.7 \\\hline
        1 & 32 & 5.5 & 6.4 & 6.5 & 7.6 & 8.4 & 10.5 & 11.0 & 17.4 \\\hline
        1 & 64 & 11.0 & 12.8 & 13.1 & 15.1 & 16.9 & 21.0 & 22.0 & 34.8 \\\hline
        1 & 128 & 22.0 & 25.6 & 26.1 & 30.2 & 33.8 & 42.0 & 44.0 & 69.6 \\\hline
        2 & 32 & 9.6 & 10.5 & 10.6 & 11.6 & 12.5 & 14.6 & 15.1 & 21.5 \\\hline
        4 & 32 & 17.8 & 18.7 & 18.8 & 19.8 & 20.7 & 22.8 & 23.3 & 29.7 \\\hline
        8 & 32 & 34.2 & 35.1 & 35.2 & 36.2 & 37.1 & 39.2 & 39.7 & 46.1 \\\hline
        4 & 64 & 60.2 & 62.0 & 62.2 & 64.3 & 66.0 & 70.1 & 71.2 & 84.0 \\\hline
    \end{tabular}
    \caption{Größe in KB über Standorte und verschiedenen Konfigurationen der ML-Modelle zur Standorterkennung.}
    \label{tab:predictions_by_loc_size}
\end{table}

\subsection{RAM}
Für den benötigten RAM muss neben dem Anteil der ML-Modelle, die Historie der Sensorwerte und die berechneten Features betrachtet werden.
Wichtig ist dabei der größte akkumulierte benötigte RAM, der zu einem Zeitpunkt benötigt werden kann.
\newline
\newline
Für jeden Sensorwert, bis auf der Detektion von WLAN-Zugangspunkten, wird ein vier Byte Datentyp angenommen.
Für die Detektion der Zugangspunkte wird ein ein Byte Datentyp angenommen.
Insgesamt beträgt der benötigte RAM für einen Vektor von Sensorwerten damit 61 Byte.
Dies setzt sich zusammen aus dem Zeitstempel, der xyz-Komponente von Accelerometer und Gyroskop, dem Lichtsensor,
der Temperatursensor, der Magnetfeldsensor, der Geräuschsensor und fünf möglichen WLAN-Zugangspunkten.
Bei einem Datenfenster von drei Einträgen wird damit 183 Byte für Sensorwerte benötigt.
\newline
\newline
Der Anteil der Features ist abhängig von den Features die für ein bestimmtes Szenario eingesetzt werden.
Insgesamt werden aber 34 Features verwendet, die vereinfacht alle als 4 Byte Datentyp angenommen werden.
Zur Evaluierung des ML-Modells wird nur die aktuelle Feature-Menge benötigt.
Damit wird für die Feature-Menge insgesamt 136 Byte benötigt, wenn alle Features verwendet werden.
\newline
\newline
Zur Ausführung eines Entscheidungswaldes wird für die Rückgabe der Wahrscheinlichkeitsverteilung für jeden Standort vier Byte benötigt.
Je nach Implementierung würde dieser Vektor mehrmals benötigt werden, z. B. bei der parallelen Evaluierung der Entscheidungsbäume skaliert dies mit der Anzahl der Prozessor.
In diesem Fall wird keine Nebenläufigkeit angenommen.
In dieser Arbeit wurden zwischen 9 und 102 Standorte untersucht, d. h. es wurden zwischen 36 und 408 Byte benötigt.
Die Anzahl der Standorte ist aber abhängig von dem Einsatzszenario.
Die anschließende Evaluierung eines Entscheidungswaldes zur Anomalieerkennung kann vernachlässigt werden,
da dieser ein diskretes Ergebnis zurückgeben kann und die benötigte Feature-Menge deutlich kleiner ist.
Damit wird für $N$ Standorte und $K$ Features mit einem Entscheidungswald als ML-Modell zu einem Zeitpunkt
ca. $183 + 4(N + K)$ Byte benötigt, d. h. bei 102 Standorten und 34 Features ca. 727 Byte.
\newline
\newline
Zur Ausführung eines KNN können nur wenige Byte verwendet werden, um die nötigen Multiplikationen eines Neuronen durchzuführen.
Dies würde die Ausführungszeit, und den Energiebedarf, aber signifikant erhöhen, da die benötigten Gewichte ständig aus dem Programmspeicher geladen werden müssen.
Das heißt, es müssen mindestens die Zwischenergebnisse einer Schicht im RAM gehalten werden, sowie ein Gewicht und und ein Bias.
Damit benötigt ein FFNN, dessen größte Schicht $M$ Neuronen hat, mindestens $4(M+2)$ Byte.
Maximal wird $4M$ Byte, zuzüglich der Größe aller Gewichte und Biase benötigt.
Der maximale RAM, der zu einem Zeitpunkt benötigt wird mit einem FFNN, beträgt damit mindestestens $183 + 4(M + K)$,
wobei $M$ die Schicht mit den meisten Neuronen von entweder dem ML-Modell zur Standort- oder Anomalieerkennung ist.

\subsection{Ausführungszeit und benötigte Energie}
Es ist sehr problemetisch eine sinvolle Estimierung für die benötigte Ausführungszeit und Energie anzugeben.
Ausführungszeit und die benötigte Energie sind abhängig von dem verwendeten Mikrocontroller.
Vergleichbare 32-Bit Mikrocontroller mit FPU (Floating Point Unit), zu den Microcontrollern die Dymel verwendet hat \cite{dymelThesis}, sind aus der AVR C Serie (TODO: Cite).
Leider ist deren Datenblätter keine Information über die Ausführungszeit von Gleitkommazahlinstruktionen und kein Energiemodell zu entnehmen.
Es ist aber anzunehmen, dass deutlich weniger Cyclen benötigt werden für hardwareunterstützte Gleitkommazahloperationen, als Software basierte Alternativen.
Aus diesem Grund wird die Ausführungszeit in Gleitkommazahl- Vergleichen, Multiplikationen und Additionen angegeben,
da diese die integralen Bestandteile der Feature-Extrahierung und Evaluation der ML-Modelle sind.
\newline
\newline
Hier werden die Konfigurationen (TODO) betrachtet, da diese die beste Klassifizierungsgenauigkeit erzielt in Relation zu ihrer Größe erzielt haben.
Die in dieser Arbeit vorgeschlagene Architektur (\ref{fig:model_idea}) hat fünf Bestandteile, die jeweils zur Gesamtausführungszeit beitragen.
Die Aufnahme der Sensorwerte wird als konstanter Energieverbrauch angenommen und in dieser Rechnung vernachlässigt.
In der ersten Feature-Extrahierung werden 34 Features aus dem Datenfenster extrahiert.
Tabelle (TODO) zeigt die estimierte Anzahl der Operationen, die pro Art des Features benötigt werden.
\newline
\newline
Tabelle (TODO) zeigt die estimierte Anzahl der Operationen, die jeweils für den besten Entscheidungswäldern und FFNNs zur Standorterkennung benötigt werden.
Die Anzahl der Vergleichsoperationen und Multiplikationen hängen nicht mit der Anzahl der Standorte zusammen,
jedoch haben sich verschieden komplexe Strukturen für diese Fälle als Beste erwiesen.
\newline
\newline
Bei der Feature-Extrahierung für das ML-Modell zur Anomalieerkennung werden nur vier Features extrahiert.
Tabelle (TODO) zeigt die estimierte Anzahl der Operationen, die für die einzelnen Features benötigt werden.
Der Entscheidungswald zur Anomalieerkennung benötigt (TODO) Vergleiche und das FFNN zur Anomalieerkennung benötigt (TODO) Multiplikationen.
\newline
\newline
Tabelle (TODO) fasst die Anzahl der Operationen für eine Konfiguration mit ausschließlich Entscheidungswäldern und FFNNs zusammen.
Es ist zu erwarten, dass die Entscheidungswälder weniger Ausführungszeit benötigen als die FFNNs, da deutlich weniger Operationen benötigt werden,
wodurch sich ein Entscheidungsbaum basierter Klasifizierer in Hinsicht auf die benötigte Energie besser eignet.
Bei konstanter Bewegung wurde in den Testszenarien der Mikrocontroller alle (TODO)s aufgeweckt.
Dies ist aber unrealistisch, da die Sensorenbox auch für lange Zeit an einem Ort verbleiben kann, was sich positiv auf den Energiebedarf auswirkt.