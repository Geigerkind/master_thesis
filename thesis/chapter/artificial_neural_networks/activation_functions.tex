\section{Aktivierungsfunktionen}
Die Aktivierungsfunktion entscheided ob ein Neuron aktiviert wird oder nicht \cite{nwankpa2018activation}.
Sie können entweder linear oder nicht-linear sein.
Es ist aber nötig nicht-lineare Funktionen zu verwenden, damit jede kontinuierliche Funktion approximiert werden kann \cite{apicella2021survey}.
Sie unterscheiden sich in ihren Eigenschaften und Berechnungskosten, was eine besondere Rolle für Mikrocontroller spielt.
\newline
\newline
In der frühen Geschichte der neuronalen Netzwerke wurde die \textit{Sigmoid}-Funktion viel verwendet, da sie asymptotisch begrenzt, kontinuierlich und nicht-linear ist \cite{apicella2021survey}.
Oft wird sie heute in der Ausgabeschicht für binäre Klassifizierungsprobleme eingesetzt \cite{nwankpa2018activation}.
Allerdings ist sie für tiefe neuronale Netzwerke ungeeignet, da der Gradient zwischen 0 und 0.25 ist und dadurch im
Backpropagation-Prozess bereits nach wenigen Schichten gegen 0 geht.
Die \textit{SoftMax}-Funktion oder normalisierte Exponentialfunktion berechnet für einen Eingabevektor eine Wahrscheinlichkeitsverteilung.
Die Einträge diese Verteilung können für die Wahrscheinlichkeiten der einzelnen Klassen eines multivariat Klassifizierungsproblem interpretiert werden.
\newline
\newline
Zur \textit{ReLU}-Familie \cite{apicella2021survey} gehört die ReLU-Funktion \cite{glorot2011deep, konda2014zero, elfwing2018sigmoid, alcaide2018swish} und
ihre Varianten \cite{maas2013rectifier}, sowie die \textit{SoftPlus}- \cite{dugas2001incorporating} und \textit{Swish}-Funktion \cite{ramachandran2017searching}.
ReLU steht für \glqq\textit{rectified linear function}\grqq\ und wird häufig in modernen neuronalen Netzwerken verwendet \cite{apicella2021survey}.
Sie ist nicht differenzierbar bei 0 und die Ableitung für negative Eingaben ist 0.
Dies kann zu \textit{sterbenden Neuronen (engl. dying neurons)} führen, da der Bias so negativ wird, sodass das Neuron nicht mehr aktiviert wird.
Zudem ist der Trainingsprozess verlangsamt, wenn der Gradient konstant 0 ist.
Dafür ist die Ableitung der Funktion ansonsten 1, was den Backpropagation-Prozess vereinfacht, da der Gradient neutral zur Aktivierungsfunktion ist.
Varianten sind beispielsweise \textit{leaky ReLU} \cite{maas2013rectifier} und \textit{ELU (exponential linear unit)} \cite{clevert2015fast},
welche versuchen die Defizite des konstanten 0 Gradienten zu lösen, indem der negative Teil der Funktion nicht 0 ist.
\textit{SoftPlus} ist analytisch, dafür aber aufwendiger zu berechnen im Vergleich zu ReLU \cite{apicella2021survey}.
Eine weitere Variante ist \textit{Swish} \cite{ramachandran2017searching}. Sie ist analytisch aber nicht monoton.
Im Vergleich zu ReLU ist sie aufwendig zu berechnen.
Ihre Autoren behaupten aber, dass dadurch bessere Ergebnisse erzielt werden können, ohne andere Parameter zu ändern.