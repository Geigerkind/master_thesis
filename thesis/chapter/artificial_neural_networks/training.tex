\section{Training von KNN}
Um die Zielfunktion $f^{*}$ zu approximieren ist eine Kostenfunktion nötig, die den Abstand von der approximierten Funktion zur approximierenden Funktion angibt \cite{nielsenneural}.
Im Optimierungsprozess wird die Kostenfunktion minimiert. 
Oft wird die Kostenfunktion auch als \textit{Verlustfunktion (engl. loss function)}, \textit{Fehlerfunktion (engl. error function)},
oder \textit{Zielfunktion (engl. objective function)} bezeichnet \cite{bengio2017deep}.
Je nach Publikation können einiger dieser Funktionen spezielle Bedeutungen haben, in dieser Arbeit haben sie aber die gleiche Bedeutung.
\newline
\newline
Das KNN wird für eine endliche Anzahl an Trainingsdurchläufen trainiert \cite{nielsenneural}.
Ein Trainingsdurchlauf einer Trainingsmenge wird als Epoche bezeichnet.
Als \textit{Backpropagation} wird der Algorithmus bezeichnet, der mit Hilfe des Lernalgorithmus den Fehler der
Kostenfunktion rückwerts durch die Schichten des KNN propagiert, wodurch die Parameter $\boldsymbol\theta$ angepasst werden.
Der Lernalgorithmus steuert dabei, wie in dieser Epoche die Parameter mit den Trainingsdaten aktualisiert werden.
Zu den Parametern gehört die Struktur des neuronalen Netzwerks, Aktivierungsfunktionen, Gewichte und Biase.
\newline
\newline
Abhängig von der Strategie des Lernalgorithmuses müssen die Gradienten rückwerts, rekursiv Schicht für Schicht zurück propagiert werden.
Ziel ist es den Fehler unter Berücksichtigung der Parameter zurück zu propagieren.
Dafür ist es nötig die Ableitungen von der Kostenfunktion zu den Parametern zu berechnen.
Gleichung \ref{formular:general_knn} zeigt, dass das Ergebnis einer Schicht die Aktivierung der gewichteten Summe des
Ergebnisses der jeweiligen vorherigen Schicht ist.
Um die Ableitungen unter Berücksichtigung der Parameter zu berechnen ist die Anwendung der Kettenregel nötig.
\newline
\newline
Angefangen mit dem Gradienten der Aktivierungen der Ausgabeschicht, wird der Fehler rekursiv zurück propagiert.
Dabei wird in jeder Schicht die Korrektur auf die Parameter addiert.
Anschließend kann der Vorgang für nächste Epoche wiederholt werden.