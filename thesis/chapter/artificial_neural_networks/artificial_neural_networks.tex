\chapter{Künstliche Neuronale Netze}
Das mathematische Modell von künstlichen neuronalen Netzen wurde von McCulloch und Pitts im Jahre 1943 erfunden \cite{mcculloch1943logical}.
Dieses Modell ist eine Abstraktion des biologischen Neuronen als logischer Mechanismus.
Man unterscheided beim biologischen Neuronen zwischen \textit{Afferent}-Neuronen, \textit{Efferent}-Neuronen und \textit{Inter}-Neuronen (TODO: Quelle).
Afferent-Neuronen nehmen elektrische Signale von Organen entgegen und können als \textit{Input} interpretiert werden.
Efferent-Neuronen geben elektrische Signale an \textit{Effektorzellen} weiter und können als \textit{Output} interpretiert werden.
Inter-Neuronen nehmen elektrische Signale von Afferent-Neuronen oder Inter-Neuronen entgegen und geben sie an Inter-Neuronen oder Efferent-Neuronen weiter.
Wenn der Schwellenwert eines \textit{Dendrite} von einem Neuronen durch ein elektrisches Signal erreicht wurde, wird ein elektrisches Signal über den \textit{Axon} an ein anderes Neuron oder Effektorzellen übertragen.
Diese Charakteristiken werden mathematisch als ein Vergleich von einer gewichtete Summe von eingehenden Signalen mit einem Schwellenwert modelliert (TODO: Quelle).
Gleichung \ref{formular:neuron_activation} stellt diesen Zusammenhang dar.
\begin{align}
    \label{formular:neuron_activation}
    y = a(\textbf{w}^\intercal\textbf{x} + b)
\end{align}
Die Vergleichsoperation ist die \textit{Aktivierungsfunktion} $a: \mathbb{R}\mapsto\mathbb{R}$, die in diesem Fall ist es die Stufenfunktion (TODO: Quelle).
Die Eingabe $\textbf{x}\in\mathbb{R}^n$ wird mit $\textbf{w}\in[0, 1]$ gewichtet und der \textit{Bias} $b\in\mathbb{R}$ wird addiert. Der Bias stellt den Schwellenwert dar.
\newline
\newline
Das künstliche neuronale Netz approximiert eine arbiträre Funktion $f^*$. Dazu findet es eine Menge von Parametern $\boldsymbol\theta$, wodurch $f^*(\textbf{x})\approx f(\textbf{x}, \boldsymbol\theta)$
möglichst gut von der Approximationsfunktion $f$ abgebildet wird.
\newline
\newline
Das KNN ist in Schichten organisiert. Analog zu den biologischen Neuron, gibt es eine \textit{Eingabeschicht (engl. Input-Layer)},
\textit{Ausgabeschicht (engl. Output-Layer)} und \textit{verdeckte Schicht (engl. Hidden-Layer)} (TODO: Quelle).
Analog zur Aktivierung eines einzelnen Neuronen, dargestellt in Gleichung \ref{formular:neuron_activation}, stellt Gleichung \ref{formular:layer_activation} die Aktivierung einer Schicht dar.
\begin{align}
    \label{formular:layer_activation}
    \textbf{a}_{i+1} = a_i(\textbf{z}_i), \hspace{2cm} \textbf{z}_i := \textbf{W}_i\textbf{a}_i + \textbf{b}_i
\end{align}
Das allgemeine KNN verfügt über $m\in\mathbb{N}$ Schichten. Jede Schicht $i$ verfügt über $n_i\in\mathbb{N}$ Neuronen.
Die Aktivierungsfunktion $a_i:\mathbb{R}^{n_{i+1}}\mapsto\mathbb{R}^{n_{i+1}}$ berechnet die Aktivierung mit der gewichteten Summe $\textbf{z}_i$.
Die gewichtete Summe setzt sich zusammen aus der Aktivierung der vorherigen Schicht $\textbf{a}_i$ die mit $W_i\in\mathbb{R}^{n_{i+1}x{n_{i}}}$ gewichtet wird.
Die Schwellenwerte jedes Neuronen werden durch die Biase $\textbf{b}_i\in\mathbb{R}^{n_{i+1}}$ dargestellt.
Gleichung \ref{formular:general_knn} zeigt, wie das allgemeine KNN mit einer rekursiven Funktion modelliert werden kann.
\begin{align}
    \label{formular:general_knn}
    \textbf{a}_1 := \textbf{x}, \hspace{1cm} \textbf{z}_i := \textbf{W}_i\textbf{a}_i + \textbf{b}_i, \hspace{1cm} \textbf{a}_{i+1} := a_i(\textbf{z}_i), \hspace{1cm} \textbf{f(x)} := \textbf{a}_m
\end{align}
Diese Arbeit nutzt ausschließelich \textit{Feed Forward neuronale Netzwerke}.
Diese werden durch \textit{dichte Schichten (engl. Dense-Layer)} charakterisiert, d. h. Schichten in denen alle Neuronen einer Schicht mit allen Neuronen der folgenden Schicht verbunden sind.

\section{Keras}
Keras ist die am meisten genutzte \textit{deep learning} API, die in Python geschrieben ist \cite{kerasDoc}.
Dadurch ist sie kompatibel mit allen gängigen Betriebssystemen.
Ihr Fokus ist eine intuitive und simple API anzubieten, sodass schnelle Iterationen im Entwicklungsprozess möglich sind.
Trotzdem ist sie effizient und skalierbar, um die Kapazitäten großer Rechenverbunde auszunutzen.
\newline
\newline
Keras abstrahiert das ML System \textit{Tensorflow}. Tensorflow implementiert ML Algorithmen, die dem Stand der Forschung entsprechen, mit dem Fokus
auf effizienten Training der Modelle \cite{abadi2016tensorflow}.
Dafür nutzt es die Multikernarchitektur von CPUs, GPUs und spezialisierte Hardware TPUs (\textbf{T}ensor \textbf{P}rocessing \textbf{U}nit).
Es wurde als open-source Projekt veröffentlicht und ist weit verbreitet.
\newline
\newline
Keras bietet die in dieser Arbeit benötigten Algorithmen an, weshalb sie zum Trainieren von FFNNs verwendet wird.

\section{Backpropagation}
\begin{itemize}
    \item Backpropagation von FFNN
    \item Gehe auch auf Verlust (loss) und Verlustfunktionen ein => Zusammenhang Optimierungsproblem
    \item Exploding/Vanishing Gradient Problem und wie es mit Aktivierungsfunktionen
\end{itemize}

\section{Aktivierungsfunktionen}
Die Aktivierungsfunktion trennt die einzelnen Schichten von einander, indem sie nicht-linearität einführt (TODO: Quelle).
Aktivierungsfunktion sollten kontinuierlich und differenzierbar sein.
Dies ist wichtig für den \textit{Backpropagation}-Prozess beim Trainieren.
Sie unterscheiden sich in ihren Eigenschaften und Berechnungskosten, was eine besondere Rolle für Mikrocontroller spielt.
Es gibt viele verschiedene Aktivierungsfunktionen, eingeteilt in Familien.
\newline
\newline
In der \textit{Heaviside}-Familie sind die Funktionen \textit{Heaviside}, \textit{modifizierte Heaviside} und die \textit{Logistik} enthalten.
Die Heaviside-Funktion, oder auch Stufenfunktion, ist die Aktivierungsfunktion des biologischen Modells.
Sie ist aber nicht kontinuierlich bei 0 und damit nicht differenzierbar.
Dies stellt ein Problem für neuronale Netze dar, da einerseits sie einerseits unsymmetrisch ist und andererseits die Information über die Distanz zur Entscheidungsgrenze verloren geht.
Die modifizierte Heavisde-Funktion löst das erste Problem, indem 0 der Wert $\frac{1}{2}$ zugeordnet wird.
Das zweite Problem kann durch \textit{sigmoidal} Funktionen gelöst werden.
Eine Variante ist die Logistikfunktion, welche in der frühen Geschichte der neuronalen Netzwerke verwendet wurde.
Sie ist kontinuierlich und differenzierbar. Der Wert der Ableitung ist in der Rechweite $[0, \frac{1}{4}]$.
Dadurch ist sie ungeeignet für tiefe neuronale Netzwerke, da dadurch der Gradient bereits nach wenigen Schichten gegen 0 geht.
Eine Generalisierung der Logistikfunktion ist die \textit{SoftMax}-Funktion oder normalisierte Exponentialfunktion.
Diese wird häufig in der Ausgabeschicht, da ihre Ausgabewerte in der Summe 1 ergeben und sie aus diesem Grund als Wahrscheinlichkeit für jede Klasse betrachtet werden können.
\newline
\newline
Zur \textit{ReLU}-Familie gehört die ReLU-Funktion und ihre Varianten, sowie die \textit{SoftPlus}- und \textit{Swish}-Funktion.
ReLU steht für \glqq\textit{rectified linear function}\grqq\ und wird häufig in modernen neuronalen Netzwerken verwendet.
Sie ist nicht differenzierbar bei 0 und die Ableitung für negative Eingaben ist 0.
Dafür ist die Ableitung der Funktion ansonsten 1, was sehr gut für den Backpropagation-Prozess ist, da dadurch der Gradient unverändert ist.
Zudem ist sie sehr leicht zu berechnen.
Varianten sind \textit{leaky ReLU} und \textit{ELU (exponential linear unit)} welche versuchen das Problem bei 0 zu lösen.
\textit{SoftPlus} ist analytisch, dafür aber aufwendiger zu berechnen im Vergleich zu ReLU.
Eine weitere Variante ist \textit{Swish}. Sie ist analytisch aber nicht monoton.
Im Vergleich zu ReLU ist sie aufwendig zu berechnen.
Ihre Autoren behaupten aber, dass dadurch bessere Ergebnisse erzielt werden können, ohne andere Parameter zu ändern.
\newline
\newline
Daneben gibt es noch die \textit{Sign}-Familie und die \textit{Abs}-Familie, die in dieser Arbeit nicht verwendet werden (TODO: Quelle).

\section{Optimierer}
TODO

\section{Ressourcenbedarf auf dem Mirkocontroller}
\begin{itemize}
    \item Energieverbrauch
    \item Speicherverbrauch
    \item RAM verbrauch
    \item Siehe Kubik und Co.
\end{itemize}